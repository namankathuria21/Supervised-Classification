{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxKwOUjk/JcEZ4hvEa5rKf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namankathuria21/Supervised-Classification/blob/main/Supervised_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4-5YRMZxjUod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Information Gain is a metric used in Decision Trees to measure how well a feature separates the training data into target classes. It is based on the concept of entropy, which measures randomness or impurity in the dataset.\n",
        "\n",
        "Information Gain is calculated as the reduction in entropy after splitting the dataset on a feature.\n",
        "The feature with the highest Information Gain is selected for splitting because it results in purer child nodes and improves classification accuracy.\n",
        "\n",
        "\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Aspect\tGini Impurity\tEntropy\n",
        "Measure\tProbability of misclassification\tMeasure of disorder\n",
        "Formula\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "2\n",
        "1‚àí‚àëp\n",
        "2\n",
        "\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "ùëù\n",
        "‚àí‚àëplog\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "Speed\tFaster to compute\tSlower to compute\n",
        "Usage\tUsed in CART\tUsed in ID3, C4.5\n",
        "Sensitivity\tLess sensitive to changes\tMore sensitive\n",
        "\n",
        "Use case:\n",
        "\n",
        "Gini is preferred when speed is important\n",
        "\n",
        "Entropy is used when precise class separation is required\n",
        "\n",
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning is a technique that stops the growth of a Decision Tree early to prevent overfitting. The tree growth is restricted by setting conditions such as:\n",
        "\n",
        "Maximum depth\n",
        "\n",
        "Minimum samples per split\n",
        "\n",
        "Minimum samples per leaf\n",
        "\n",
        "This helps reduce complexity, training time, and overfitting."
      ],
      "metadata": {
        "id": "yoJu6Fk2jQOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity and print feature importances."
      ],
      "metadata": {
        "id": "IgKbfvTSjXx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zfXghlHjfCQ",
        "outputId": "12b94f4c-05a7-4e8f-aa38-027c52501ff0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.01333333 0.55072262 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gpC1Fflaj8Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uV6TIeB1j-9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. It works by finding an optimal hyperplane that best separates data points of different classes with the maximum margin.\n",
        "\n",
        "The data points closest to the hyperplane are called support vectors, and they play a key role in defining the decision boundary.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "The Kernel Trick allows SVMs to handle non-linear data by transforming it into a higher-dimensional space where a linear separation is possible.\n",
        "\n",
        "Common kernels include:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "Radial Basis Function (RBF)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "It avoids explicit computation in higher dimensions, making SVM efficient.\n",
        "\n",
        "Question 7: Write a Python program to train Linear and RBF SVM classifiers on the Wine dataset and compare accuracy."
      ],
      "metadata": {
        "id": "NoqeevkCjtIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0Y2OMiIjx7q",
        "outputId": "2a3fa59c-99b5-42ab-81f1-23a2f530247e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "Answer:\n",
        "Na√Øve Bayes is a probabilistic classification algorithm based on Bayes‚Äô Theorem. It assumes that all features are independent of each other, which is rarely true in real-world data.\n",
        "\n",
        "It is called \"Na√Øve\" because of this strong independence assumption, yet it performs surprisingly well in many applications such as text classification and spam detection.\n",
        "Question 9: Explain the differences between Gaussian, Multinomial, and Bernoulli Na√Øve Bayes.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Type\tData Type\tUse Case\n",
        "Gaussian NB\tContinuous data\tMedical, numerical data\n",
        "Multinomial NB\tCount-based data\tText classification\n",
        "Bernoulli NB\tBinary data\tYes/No features\n",
        "\n",
        "Each variant is chosen based on the nature of the dataset.\n",
        "\n",
        "Question 10: Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "uRGrp2sjkAfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssARrFi3kJzO",
        "outputId": "4a8a781e-4e61-4123-ea14-68c07ced22d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}